"""
Silero VADå‡¦ç†ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½¿ç”¨æ–¹æ³•: python test_vad.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>
"""

import os
import sys
import torch
import torchaudio
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from dental_processor import process_with_vad, load_silero_vad

def visualize_speech_segments(audio_tensor, timestamps, sample_rate=16000):
    """éŸ³å£°æ³¢å½¢ã¨æ¤œå‡ºã•ã‚ŒãŸç™ºè©±åŒºé–“ã‚’å¯è¦–åŒ–"""
    fig, ax = plt.subplots(figsize=(15, 5))
    
    # æ™‚é–“è»¸ï¼ˆç§’ï¼‰
    time = np.arange(len(audio_tensor)) / sample_rate
    
    # æ³¢å½¢ãƒ—ãƒ­ãƒƒãƒˆ
    ax.plot(time, audio_tensor, color='gray', alpha=0.5)
    
    # ç™ºè©±åŒºé–“ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    for ts in timestamps:
        start_time = ts['start'] / sample_rate
        end_time = ts['end'] / sample_rate
        duration = end_time - start_time
        ax.axvspan(start_time, end_time, color='green', alpha=0.3)
        ax.text(start_time + duration/2, 0, f"{duration:.2f}s", 
                horizontalalignment='center', verticalalignment='center')
    
    ax.set_title('éŸ³å£°æ³¢å½¢ã¨æ¤œå‡ºã•ã‚ŒãŸç™ºè©±åŒºé–“')
    ax.set_xlabel('æ™‚é–“ (ç§’)')
    ax.set_ylabel('æŒ¯å¹…')
    
    # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€
    output_dir = 'output'
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f'speech_segments_{timestamp}.png')
    
    # ä¿å­˜
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"å¯è¦–åŒ–çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    
    # å¯èƒ½ãªã‚‰ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º
    try:
        plt.show()
    except:
        print("ãƒ—ãƒ­ãƒƒãƒˆã®è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼ˆGUIç’°å¢ƒãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")

def convert_to_wav_if_needed(file_path):
    """å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’WAVã«å¤‰æ›"""
    if file_path.lower().endswith(('.m4a', '.mp3', '.aac', '.ogg')):
        import tempfile
        import subprocess
        import os
        
        print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’WAVã«å¤‰æ›ä¸­: {file_path}")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        temp_dir = tempfile.gettempdir()
        temp_wav = os.path.join(temp_dir, f"temp_{os.path.basename(file_path)}.wav")
        
        # FFmpegã‚’ä½¿ç”¨ã—ã¦å¤‰æ›
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®FFmpegã‚’æ¢ã™
            ffmpeg_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "ffmpeg.exe")
            if not os.path.exists(ffmpeg_path):
                ffmpeg_path = "ffmpeg"  # ã‚·ã‚¹ãƒ†ãƒ ã®FFmpegã‚’ä½¿ç”¨
                
            command = [ffmpeg_path, "-i", file_path, "-ac", "1", "-ar", "16000", temp_wav, "-y"]
            subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            print(f"å¤‰æ›å®Œäº†: {temp_wav}")
            return temp_wav
        except Exception as e:
            print(f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            print("å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return file_path
    return file_path

def test_vad_processing(audio_file):
    """VADå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’è¡¨ç¤º"""
    print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {audio_file}")
    
    if not os.path.exists(audio_file):
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_file}")
        return False
    
    # å¿…è¦ã«å¿œã˜ã¦WAVã«å¤‰æ›
    converted_file = convert_to_wav_if_needed(audio_file)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: VADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print("1. VADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model, get_speech_timestamps, read_audio = load_silero_vad()
    
    if model is None:
        print("ã‚¨ãƒ©ãƒ¼: VADãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    print("2. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        audio_tensor = read_audio(converted_file, sampling_rate=16000)
        print(f"   éŸ³å£°é•·: {len(audio_tensor)/16000:.2f}ç§’ ({len(audio_tensor)}ã‚µãƒ³ãƒ—ãƒ«)")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ç™ºè©±åŒºé–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
    print("3. ç™ºè©±åŒºé–“ã‚’æ¤œå‡ºä¸­...")
    timestamps = get_speech_timestamps(
        audio_tensor, 
        model, 
        threshold=0.5,
        sampling_rate=16000,
        min_silence_duration_ms=700,
        speech_pad_ms=300
    )
    
    print(f"   æ¤œå‡ºã•ã‚ŒãŸç™ºè©±åŒºé–“: {len(timestamps)}å€‹")
    
    # çµæœã®è¡¨ç¤º
    total_speech_duration = 0
    for i, ts in enumerate(timestamps):
        start_sec = ts['start'] / 16000
        end_sec = ts['end'] / 16000
        duration = end_sec - start_sec
        total_speech_duration += duration
        print(f"   åŒºé–“ {i+1}: {start_sec:.2f}ç§’ - {end_sec:.2f}ç§’ (é•·ã•: {duration:.2f}ç§’)")
    
    total_duration = len(audio_tensor) / 16000
    print(f"\nç·éŸ³å£°é•·: {total_duration:.2f}ç§’")
    print(f"ç™ºè©±éƒ¨åˆ†: {total_speech_duration:.2f}ç§’ ({total_speech_duration/total_duration*100:.1f}%)")
    print(f"ç„¡éŸ³éƒ¨åˆ†: {total_duration-total_speech_duration:.2f}ç§’ ({(total_duration-total_speech_duration)/total_duration*100:.1f}%)")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: å¯è¦–åŒ–
    print("\n4. çµæœã‚’å¯è¦–åŒ–ä¸­...")
    visualize_speech_segments(audio_tensor, timestamps)
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: process_with_vadé–¢æ•°ã®ãƒ†ã‚¹ãƒˆ
    print("\n5. process_with_vadé–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    processed_file = process_with_vad(converted_file)
    print(f"   å‡¦ç†å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«: {processed_file}")
    
    if processed_file == converted_file:
        print("   è­¦å‘Š: å‡¦ç†å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜ã§ã™ã€‚VADå‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        print("   æˆåŠŸ: VADå‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Silero VADå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ")
    parser.add_argument("audio_file", help="ãƒ†ã‚¹ãƒˆç”¨ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    args = parser.parse_args()
    
    if not args.audio_file:
        print("ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        print("ä½¿ç”¨æ–¹æ³•: python test_vad.py <éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        sys.exit(1)
    
    print("======= Silero VADå‡¦ç†ãƒ†ã‚¹ãƒˆ =======")
    success = test_vad_processing(args.audio_file)
    
    if success:
        print("\nãƒ†ã‚¹ãƒˆå®Œäº†! ğŸ‰")
    else:
        print("\nãƒ†ã‚¹ãƒˆå¤±æ•— ğŸ˜¢")
        sys.exit(1) 